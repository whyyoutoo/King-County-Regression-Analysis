{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Wesley Yu\n",
    "* Student pace: Flex\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enter image here?\n",
    "\n",
    "![example](images/director_shot.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County Home Regression Analysis\n",
    "\n",
    "**Authors:** Wesley Yu\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project will use multiple linear regression to analyze house sales in King County WA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "A real estate company is looking to expand into the King County market. They want to know what kind of features will affect house prices in this area so that they can advise potential home buyers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Data set from King County contains 21,597 records of house sales during 2014 - 2015 period. Data also contains sale prices, various features describing the homes, condition of property when sold, and location details. Additional data from US Post office was added to help narrow down zipcodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### EDA \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import folium\n",
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/kc_house_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df.duplicated(subset = 'id').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(subset = 'id', keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset = 'id', keep = 'last', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates were found to be homes that were sold twice during 2014-2015 period. I decided to keep most recent records of house sales to reflect the changing market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nulls\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.waterfront.fillna('NO', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.waterfront.replace({'NO': 0, 'YES':1}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to replace null values in waterfall to 'NO' and convert binary values for inputting into model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.view.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.view.fillna('NONE', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.view.replace({'NONE': 0, 'FAIR': 1, 'AVERAGE': 2, 'GOOD':3, 'EXCELLENT': 4}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same was done to view, and replaced with ordinal values with numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.yr_renovated.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('yr_renovated', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large number of nulls in the yr_renovated feature. Majority of values are also 0. We will drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sqft_basement.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sqft_basement.replace('?', 0, inplace = True)\n",
    "df.sqft_basement = df.sqft_basement.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.sqft_basement + df.sqft_above == df.sqft_living).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.sqft_basement + df.sqft_above != df.sqft_living), 'sqft_basement'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.sqft_basement > 0, 'sqft_basement'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'sqft_basement': 'basement'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing '?' with 0. It looks like sqft_basement + sqft_above = sqft_living for almost every value. Changed sqft_basement to new column with binary values, 0 for no basement and 1 for basement. The 170 records where sqft_basement + sqft_above != sqft_living will be assign to 1 (has basement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.replace({'Poor': 1, 'Fair': 2, 'Average': 3, 'Good':4, 'Very Good': 5}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition values changed from ordinal to numeric for modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.grade = df.grade.str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grade feature we will keep only digit values and convert to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick summary shows an odd outlier for bedrooms. Upon examination this could be a typo, I changed value to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.bedrooms == 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.bedrooms == 33, 'bedrooms'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bathrooms.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bathrooms = df.bathrooms.map(np.ceil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bathrooms.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decide to treat 1/4, 1/2, and 3/4 bathrooms as whole bathrooms, for easier interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a quick look at distribution of each variable\n",
    "df.hist(figsize = (20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model optimization, i decide to remove any values in feature that represent less than 1% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bedrooms.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df.bedrooms < 2) | (df.bedrooms > 6)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bathrooms.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.bathrooms > 4].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.condition.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.condition < 3].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.grade.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[(df.grade < 5) | (df.grade > 11)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.sqft_lot > 200000).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.sqft_lot > 200000].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.zipcode.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.zipcode.isin(counts[counts < 0.01].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize = (20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a map with all records of houses marked, to get an idea if price is affected by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting coordinates for the center of map\n",
    "centerlat = (df['lat'].max() + df['lat'].min()) / 2\n",
    "centerlong = (df['long'].max() + df['long'].min()) / 2\n",
    "center = (centerlat, centerlong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a LinearColorMap and assign colors, vmin, and vmax\n",
    "# the colormap will show green for $100,000 homes all the way up to red for $1,500,000 homes\n",
    "colormap = cm.LinearColormap(colors=['green', 'yellow', 'red'], vmin=100000, vmax=1500000)\n",
    "\n",
    "# create our map again.  This time I am using a different tileset for a new look\n",
    "m = folium.Map(location=center, zoom_start=10, tiles = 'Stamen Toner')\n",
    "\n",
    "# Same as before... go through each home in set, make circle, and add to map.\n",
    "# This time we add a color using price and the colormap object\n",
    "for i in range(len(df)):\n",
    "    folium.Circle(\n",
    "        location=[df.iloc[i]['lat'], df.iloc[i]['long']],\n",
    "        radius=10,\n",
    "        fill=True,\n",
    "        color=colormap(df.iloc[i]['price']),\n",
    "        fill_opacity=0.2\n",
    "    ).add_to(m)\n",
    "\n",
    "# the following line adds the scale directly to our map\n",
    "m.add_child(colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see location has an effect of the price. Further south of Seattle shows the largest area of cheaper houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column representing age of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)\n",
    "df['house_age'] = df.date.dt.year - df.yr_built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help cut down the size of unique zipcode variables, I bring in data on which cities the zipcodes belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.zipcode.value_counts().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/zip_code_database.csv')\n",
    "zip_king = data[(data.county == 'King County') & (data.state == 'WA') & (data['type'] == 'STANDARD')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_king.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_cities = zip_king[['zip', 'primary_city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, king_cities, left_on='zipcode', right_on='zip', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop = ['id', 'date', 'lat', 'long', 'zipcode', 'zip', 'yr_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(todrop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a great method to determine the strength of predictors. In order for linear regression to produce trustworthy results there are certain assumptions that need to be followed: \n",
    "* Linearity - relationship between predictor and target should be linear.\n",
    "* Independence - Observations are independent from each other, low or no multicollinearity.\n",
    "* Normality - Errors should be normally distributed\n",
    "* Homoscedasticity - variance or the errors is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check the assumption of linearity by inspecting the scatter plots of the predictors with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = df.drop('price', axis = 1)\n",
    "y = df.price\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=6, figsize=(10, 20))\n",
    "fig.set_tight_layout(True)\n",
    "fig.delaxes(axes[5,1])\n",
    "\n",
    "for index, col in enumerate(x.columns):\n",
    "    ax = axes[index//2][index%2]\n",
    "    ax.scatter(x[col], y, alpha=0.2)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear relationship is not visible for house_age, bedrooms, floors, sqft_lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking with boxplots for clarity shows linear relationship with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'bedrooms', y = 'price', data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'view', y = 'price', data = df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'floors', y = 'price', data = df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation of price and sqft_living improves linearity, will apply this during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.log(df.sqft_living)\n",
    "y = np.log(df.price)\n",
    "sns.scatterplot(x=x,y=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will check for multicollinearity between our predictors. This can be done by inspecting correlation of each predictor. Using a heatmap of the correlation matrix is a helpful visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.drop('price', axis = 1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating heatmap of correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "sns.heatmap(data=corr, mask=np.triu(np.ones_like(corr, dtype=bool)), ax=ax,\\\n",
    "            annot=True,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally 0.7 - 0.8 or higher absolute value of correlation is considered high. We will take anything above 0.75 as a cut-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pairs based on their correlation values\n",
    "corr_list = corr.abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "corr_list['pairs'] = list(zip(corr_list.level_0, corr_list.level_1))\n",
    "corr_list.set_index(['pairs'], inplace = True)\n",
    "corr_list.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "corr_list.columns = ['cc']\n",
    "corr_list.drop_duplicates(inplace=True)\n",
    "corr_list[(corr_list.cc > .75) & (corr_list.cc < 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqft_above, sqft_living, sqft_lot15, sqft_lot, and sqft_living15 are found to be highly correlated variables. \n",
    "Removing all but sqft_living and sqft_lot should help with multicollinearity problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['sqft_above', 'sqft_lot15', 'sqft_living15'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start our baseline model we will select variables that have the highest correlation with our target price, which is found to be sqft_living and grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().price.abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data set to train/test data sets for model validation later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price', axis = 1)\n",
    "y = df.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split categorical variables from numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = X_train.drop('primary_city', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat = X_train.primary_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas to create dummy variables for primary_city. Dropping a column to avoid multicolinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(X_train_cat)\n",
    "dummies.drop('Seattle', axis = 1, inplace = True)\n",
    "X_train_cat = pd.concat([X_train_cat, dummies], axis = 1)\n",
    "X_train_cat.drop('primary_city', axis = 1, inplace = True)\n",
    "X_train_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model will consist of the 2 highest correlated variables to our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr = ['sqft_living', 'grade']\n",
    "firstX = X_train_num[high_corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y_train, sm.add_constant(firstX)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 value of 0.488 shows that only around 49% of prices variance is explained by sqft living and grade. \n",
    "p-values for all predictors round to 0, which indicates that they have statistical significance.\n",
    "coef for sqft_living shows for each unit of increased in sqft_living, price will increase by 151.25\n",
    "same for grade with an increase of 98,130 in price for each unit of grade increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for normality of residuals with QQ plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(sm.add_constant(firstX))\n",
    "residuals = model.resid\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for homoscedasticity in residuals with scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(sm.add_constant(firstX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(x = y_pred, y = residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions for normality and homoscedasticity are found to be false. This can be corrected by log transformation of the target 'price', as its distribution was right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "firstX_log = pd.DataFrame({})\n",
    "firstX_log['grade'] = X_train_num.grade\n",
    "firstX_log['sqft_living_log'] = np.log(X_train_num.sqft_living)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log = sm.OLS(y_train_log, sm.add_constant(firstX_log)).fit()\n",
    "model_log.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After log transformation of our target coef for predictors are now in terms of log.\n",
    "Each percent of unit increase of sqft_living will increase price by 0.42%\n",
    "unit increase in grade will result in a 19% increase in price.\n",
    "R^2 value has also increased a bit after the transformation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_log.predict(sm.add_constant(firstX_log))\n",
    "residuals = model_log.resid\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_log.predict(sm.add_constant(firstX_log))\n",
    "sns.residplot(x = y_pred, y = residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions for normality and homoscedasticity are now satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding moderately correlated variables to see if we can increase R^2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = pd.concat([y_train, X_train_num], axis=1)\n",
    "corr_data.corr().price.abs().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderate = ['bathrooms', 'view', 'waterfront', 'bedrooms']\n",
    "secondX = pd.concat([firstX_log, X_train_num[moderate]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = sm.OLS(y_train_log, sm.add_constant(secondX)).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 has increased a bit to 0.534\n",
    "p-values round to 0\n",
    "assumptions for normality and homoscedasticity still hold.\n",
    "Interestingly additional units of bedrooms and bathrooms decrease the price by around 4% for bathrooms and 2% bedrooms if sqft_living remains the same. We will add on our variables to account for location in next model to see if this changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model2.predict(sm.add_constant(secondX))\n",
    "residuals = model2.resid\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(sm.add_constant(secondX))\n",
    "sns.residplot(x = y_pred, y = residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in our dummy encoded variables to see if location can account for any variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thirdX = pd.concat([secondX, X_train_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = sm.OLS(y_train_log, sm.add_constant(thirdX)).fit()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R^2 score has increased to 0.737\n",
    "p=value for bathrooms now 0.355 indicating that this predictor may not be significant, we can consider dropping for next model.\n",
    "p-value for federal way also showing to be barely non significant at 0.057.\n",
    "Since we dropped Seattle dummy variable. coeffiencents for listed cities are in reference to Seattle. \n",
    "the price of a similar house in Auburn is around 58% cheaper than seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model3.predict(sm.add_constant(thirdX))\n",
    "residuals = model3.resid\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model3.predict(sm.add_constant(thirdX))\n",
    "sns.residplot(x = y_pred, y = residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually going through models of different variables. We can take advantage of RFE, recurrsive feature elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.sqft_living = np.log(X_train_num.sqft_living)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfeX = pd.concat([X_train_num, X_train_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_rfe = LinearRegression()\n",
    "select = RFE(lr_rfe, n_features_to_select=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select.fit(X=rfeX, y=y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = select.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalX = rfeX[rfeX.columns[features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = sm.OLS(y_train_log, sm.add_constant(finalX)).fit()\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "strong R^2 of 0.747, adjusted R^2 also high. \n",
    "p-values of all predictors round to 0\n",
    "Strongest predictors show to be location based. followed by grade and condition of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(model4.mse_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model4.predict(sm.add_constant(finalX))\n",
    "residuals = model4.resid\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model4.predict(sm.add_constant(finalX))\n",
    "sns.residplot(x = y_pred, y = residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_var = pd.concat([X_train_num, X_train_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = sm.OLS(y_train_log, sm.add_constant(all_var)).fit()\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although R^2 score is higher in this model, condition number is very high as well. this is an indicator that there may be strong multicollinearity between the predictors and may affect the coefficient values. We will go with model 4 as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step is to test our model with new data and see how well it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_log = np.log(y_test)\n",
    "X_test.sqft_living = X_test.sqft_living.map(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num = X_test.drop('primary_city', axis = 1)\n",
    "X_test_cat = X_test.primary_city\n",
    "\n",
    "dummies = pd.get_dummies(X_test_cat)\n",
    "dummies.drop('Seattle', axis = 1, inplace = True)\n",
    "\n",
    "X_test_final = pd.concat([X_test_num, dummies], axis = 1)\n",
    "X_test_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltestX = X_test_final[X_test_final.columns[features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model4.predict(sm.add_constant(finalX))\n",
    "y_pred_test = model4.predict(sm.add_constant(finaltestX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(y_train_log, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test_log, y_pred_test)\n",
    "print('Train Mean Squarred Error:', train_mse)\n",
    "print('Test Mean Squarred Error:', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for train and test do not show a big difference, which indicates our model performed similarly on the new data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "\n",
    "\n",
    "- __Location, location, location__ Our final model showed that the biggest factor in house prices are location. results from analysis show as you move futher south(away) from Seattle, prices for a house with similar features decrease.\n",
    "\n",
    "- __House Quality__ Aside from location, the next biggest factor in house prices are the condition and grade of the property.\n",
    "\n",
    "- __Limitations__ \n",
    "\n",
    " \n",
    "## Next Steps\n",
    "\n",
    "- __House Features__ We can group data by city only and see if specific features of house ie. Number of bathrooms or bedrooms have an affect on house price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "490.85px",
    "left": "706px",
    "right": "20px",
    "top": "104px",
    "width": "581px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
